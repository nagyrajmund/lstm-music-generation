#!/usr/bin/env python
# coding: utf-8

# In[ ]:


# A simplified version of AWD-LSTM implment from scratch, code adapted from https://github.com/a-martyn/awd-lstm 


# In[2]:


import math
import torch 
import torch.nn as nn
import torch.nn.functional as F
from torch import Tensor 


# In[3]:


class LSTMCell(nn.Module):

    def __init__(self, input_size, output_size, bias=True):
        super(LSTMCell, self).__init__()
        
        # Contains all weights for the 4 linear mappings of the input x
        # e.g. Wi, Wf, Wo, Wc
        self.i2h = nn.Linear(input_size, 4*output_size, bias=bias)
        # Contains all weights for the 4 linear mappings of the hidden state h
        # e.g. Ui, Uf, Uo, Uc
        self.h2h = nn.Linear(output_size, 4*output_size, bias=bias)
        self.output_size = output_size


    def forward(self, x, hidden):
        # unpack tuple (recurrent activations, recurrent cell state)
        h, c = hidden

        # Linear mappings : all four in one vectorised computation
        preact = self.i2h(x) + self.h2h(h)

        # Activations
        i = torch.sigmoid(preact[:, :self.output_size])                      # input gate
        f = torch.sigmoid(preact[:, self.output_size:2*self.output_size])    # forget gate
        g = torch.tanh(preact[:, 3*self.output_size:])                       # cell gate
        o = torch.sigmoid(preact[:, 2*self.output_size:3*self.output_size])  # ouput gate


        # Cell state computations: 

        c_t = torch.mul(f, c) + torch.mul(i, g)
        h_t = torch.mul(o, torch.tanh(c_t))

        return h_t, c_t


# In[4]:


class WeightDropout(nn.Module):
    

    def __init__(self, module:nn.Module, weight_p:float):
        super().__init__()
        self.module, self.weight_p = module, weight_p
            
        w = getattr(self.module.h2h, 'weight')
        self.register_parameter('weight_raw', nn.Parameter(w.data))
        self.module.h2h._parameters['weight'] = F.dropout(w, p=self.weight_p, training=False)

    def _setweights(self):
        
        "Apply dropout to the raw weights."
        raw_w = getattr(self, 'weight_raw')
        self.module.h2h._parameters['weight'] = F.dropout(raw_w, p=self.weight_p, training=self.training)

    def forward(self):
        self._setweights()
        return self.module.forward()


# In[ ]:


class AWD_LSTM(nn.Module):


    def __init__(self, input_size, embedding_size, hidden_size, bias=True, device='cpu',
                 dropout_wts=0.5):
        
        super(AWD_LSTM, self).__init__()
        
        self.nlayers = 3
        self.hidden_size = hidden_size
        self.device = device
        self.embedding_size = embedding_size

        self.embedding = nn.Embedding(input_size, embedding_size)
        self.layer0 = LSTMCell(embedding_size, hidden_size, bias=bias)
        self.layer0 = WeightDropout(self.layer0, dropout_wts)
        self.layer1 = LSTMCell(hidden_size, hidden_size, bias=bias)
        self.layer1 = WeightDropout(self.layer1, dropout_wts)
        self.layer2 = LSTMCell(hidden_size, embedding_size, bias=bias)
        self.layer2 = WeightDropout(self.layer2, dropout_wts)
        self.decoder = nn.Linear(embedding_size, ntokens)
        
        self.output = None


    def forward(self, x, hiddens):

        x = self.embedding_dropout(self.embedding, x, p=self.dropout_emb)
        
        h, c = hiddens
        output = Tensor().to(self.device)
        
        # Propagate through layers for each timestep
        for t in range(x.size(0)):    
            
            inp    = x[t,:,:]
            h0, c0 = self.layer0(inp, (h[0], c[0]))
            h1, c1 = self.layer1(h0, (h[1], c[1]))
            h2, c2 = self.layer2(h1, (h[2], c[2]))
            
            h = [h0, h1, h2]
            c = [c0, c1, c2]
            
            output = th.cat((output_nodrop, h2.unsqueeze(0)))
            
        self.output = output.detach()
        
        # Translate embedding vectors to tokens
        reshaped = output.view(output.size(0)*output.size(1), output.size(2))
        decoded = self.decoder(reshaped)
        decoded = decoded.view(output.size(0), output.size(1), decoded.size(1))

        return decoded, (h, c)


# In[ ]:





# In[ ]:



